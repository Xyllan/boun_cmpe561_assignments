{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPE561: Natural Language Processing\n",
    "# Assignment 2: Part-of-Speech Tagging using Hidden Markov Models\n",
    "\n",
    "**Name:** Doruk Kilitçioğlu  \n",
    "**Student ID:** 2012400183\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment felt very informative and satisfactory to implement. The algorithms were crystal clear, and they produced results immediately. The code and the report in Jupyter Notebook format can be found at [GitHub](https://github.com/Xyllan/boun_cmpe561_assignments).\n",
    "\n",
    "Below are the incremental steps I have taken during the assignment.\n",
    "\n",
    "## 2. CoNLL Parsing\n",
    "Before I could do anything, I had to be able to parse the input files. I created a parser that would take a CoNLL file, and return its sentences. Each word of the sentence is a tuple of word form, cpostag, and postag. The words which have form fields of '_' are ignored as per the instructor's instructions, all throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hayır', 'Adv', 'Adv'), (',', 'Punc', 'Punc'), ('şarklı', 'Noun', 'Zero'), ('değil', 'Verb', 'Verb'), ('.', 'Punc', 'Punc')]\n"
     ]
    }
   ],
   "source": [
    "import conll_parser as cpar\n",
    "\n",
    "path = 'metu_sabanci_cmpe_561/train/turkish_metu_sabanci_train.conll'\n",
    "train_sentences = cpar.get_sentences(path)\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looping over the data, we get a list of cpostags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Noun', 'Interj', 'Num', 'Ques', 'Zero', 'Dup', 'Verb', 'Adj', 'Adv', 'Punc', 'Conj', 'Det', 'Pron', 'Postp'}\n"
     ]
    }
   ],
   "source": [
    "print(cpar.tag_list(train_sentences, tag_ind = cpar.tag_ind('cpostag')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a list of postags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Card', 'Verb', 'Conj', 'NPastPart', 'Ord', 'PersP', 'DemonsP', 'ReflexP', 'Noun', 'Interj', 'APastPart', 'Dup', 'Adj', 'NInf', 'Punc', 'Range', 'NFutPart', 'Ques', 'Adv', 'Prop', 'AFutPart', 'Postp', 'Distrib', 'Real', 'APresPart', 'Num', 'Zero', 'QuesP', 'Det', 'Pron'}\n"
     ]
    }
   ],
   "source": [
    "print(cpar.tag_list(train_sentences, tag_ind = cpar.tag_ind('postag')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM PoS Tagger\n",
    "The HMM PoS Tagger is a basic bi-gram part-of-speech tagger. The PoS tags are hidden, while the actual words are observable. We need to estimate the transition probabilities of states and observation likelihoods words in the given states, using the training data. Since we are using a bi-gram model, the transition probability only depends on the previous tag. The transition probability from tag $T_{t-1}$ to tag $T_t$ is estimated by:\n",
    "$$P(T_t|T_{t-1}) = \\frac{C(T_{t-1},T_t)}{C(T_{t-1})}$$\n",
    "where $C$ is the number of occurences of the given tuple in the training data.\n",
    "\n",
    "Similarly, the observation likelihood of any word $W_t$ given a tag $T_t$ is estimated by:\n",
    "$$P(W_t|T_t) = \\frac{C(W_t,T_t)}{C(T_t)}$$\n",
    "\n",
    "*Training* the HMM PoS tagger therefore refers to calculating these probabilities for all words and tags. Since that creates two very sparse matrices, I decided to keep the actual counts in memory and do the calculations online.\n",
    "\n",
    "During training, I also calculate the counts regarding the start and end states, since they are used in the Viterbi algorithm for calculating the highest likelihood tag set.\n",
    "\n",
    "## 4. Task 1\n",
    "After building the training portion of the aforementioned PoS tagger, the **train_hmm_tagger** program is pretty much finished. The instructions to run are in the README, but it is suffice to say that the program reads a training file, calculates the counts, and stores them to be used in the second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv count: 2673\n",
      "Adv to Punc count: 390\n",
      "'Hayır' as Adv count: 5\n"
     ]
    }
   ],
   "source": [
    "import train_hmm_tagger as hmm_train\n",
    "\n",
    "tag_ind = cpar.tag_ind('cpostag')\n",
    "hmm = hmm_train.HMM(cpar.tag_list(train_sentences, tag_ind), tag_ind)\n",
    "hmm.train(train_sentences)\n",
    "\n",
    "print('Adv count:',hmm.tag_count('Adv'))\n",
    "print('Adv to Punc count:',hmm.tag_pair_count('Adv','Punc'))\n",
    "print('\\'Hayır\\' as Adv count:',hmm.word_tag_count('Hayır','Adv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the two tasks separately, the trained HMM model is actually saved to a file called **hmm.conf**. This is not necessary here as we can keep everything in memory.\n",
    "\n",
    "## 5. Task 2\n",
    "In the second task, we implement the Viterbi algorithm. The algorithm is not explained in detail for the sake of brevity, and it already is properly commented in the code. In short we find the highest likelihood path from the start node to the end node, using the transition probability and the observation likelihood at each step. Since the probability of observing no words at the beginning and end states is 1, they are omitted. We say that the likelihood of a given tag list to the given word observations is *proportional to*:\n",
    "$$P(END|T_n)\\prod_{t=1}^n P(T_t|T_{t-1})P(W_t|T_t)$$\n",
    "where $t=1..n$ is the word index, and $T_0 = START$.\n",
    "\n",
    "When expressed in log-space, the log-likelihood becomes a sum of the individual log transitions and log observation likelihoods. We find the highest likelihood state sequence and use it as our predicted PoS tag sequence.\n",
    "\n",
    "One thing of note is how we handle a probability of 0 in the tagger. For each word in the sentence, we calculate the logarithm $P(T_t|T_{t-1})P(W_t|T_t)$ value. If that joined probability is zero (if the log probability is negative infinity), we plug in the value $-10^{10}$.If we have 0 probability for all possible word/tag combinations when trying to choose a tag, we would end up with a probability of 0 for the rest of the tagging, and that would mean we would be choosing random tags (since their cumulative probabilities are all the same). Instead, we are using a value where the rest of the calculations still affect the cumulative probability, and since the value is smaller than what any other calculations will produce, it does not disrupt our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kalabalıktaki', 'Noun', 'Noun'), ('insanlar', 'Noun', 'Noun'), ('da', 'Conj', 'Conj'), ('onu', 'Pron', 'Pron'), ('dinlermiş', 'Noun', 'Noun'), ('.', 'Punc', 'Punc')]\n"
     ]
    }
   ],
   "source": [
    "import hmm_tagger\n",
    "\n",
    "validation_path = 'metu_sabanci_cmpe_561/validation/turkish_metu_sabanci_val.conll'\n",
    "test_sentences = cpar.get_sentences(validation_path)\n",
    "pt_sentences = hmm_tagger.pos_tag(hmm, test_sentences)\n",
    "\n",
    "print(pt_sentences[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Unknown Words\n",
    "For unknown words, I tried two basic approaches:\n",
    "* I assigned the most common tag to the unknown words. In most cases, this tag ended up being NOUN. While there definitely are some heuristics we are missing with this approach, it works very well compared to the random approach.\n",
    "* I looked at the minimum edit (Levenshtein) distance between the vocabulary and the unknown word. This felt like it would be prone to false corrections, and it took way too much time (even when using dynamic programming) to go through the whole vocabulary each time. The results are not written here for the sake of brevity, but they are not an improvement over the most common tag solution, and take exponentially more time (~10 minutes versus the most common tag approach's ~1 second).\n",
    "\n",
    "## 6. Task 3 & Results\n",
    "The evaluation part is very straightforward and is pretty much a copy of the evaluation scheme of the first assignment (just calculating accuracy instead of the other indicators), so I won't be talking about the implementation. Between task 2 and task 3, the output is written to a file, but that is not necessary in this report since we can keep them in memory.\n",
    "\n",
    "The only thing of significant change from the other assignment is that the stats for known and unknown words are separated, so that we can report them separately.\n",
    "\n",
    "### 6.1 CPOSTAG Results\n",
    "#### 6.1.1 Unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "Overall Accuracy: 0.662172878668\n",
      "Noun Accuracy: 0.662172878668\n",
      "Interj Accuracy: 1.0\n",
      "Num Accuracy: 0.990483743061\n",
      "Ques Accuracy: 1.0\n",
      "Zero Accuracy: 1.0\n",
      "Dup Accuracy: 1.0\n",
      "Verb Accuracy: 0.805709754163\n",
      "Adj Accuracy: 0.903251387787\n",
      "Adv Accuracy: 0.965107057891\n",
      "Punc Accuracy: 1.0\n",
      "Conj Accuracy: 1.0\n",
      "Det Accuracy: 1.0\n",
      "Pron Accuracy: 0.998413957177\n",
      "Postp Accuracy: 0.999206978588\n",
      "Confusion matrix:\n",
      "  Tags: ['Noun', 'Interj', 'Num', 'Ques', 'Zero', 'Dup', 'Verb', 'Adj', 'Adv', 'Punc', 'Conj', 'Det', 'Pron', 'Postp']\n",
      "[[835   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 12   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [245   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [122   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 44   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from evaluate_hmm_tagger import Tester\n",
    "\n",
    "t = Tester(hmm.tags)\n",
    "t.build(test_sentences, pt_sentences, hmm.tag_ind, vocab = hmm.vocab)\n",
    "\n",
    "t.print_acc(0)\n",
    "t.print_conf(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, due to the high number of nouns, putting in nouns where we don't know the word works surprisingly well.\n",
    "\n",
    "#### 6.1.2 Known Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "Overall Accuracy: 0.948063005534\n",
      "Noun Accuracy: 0.977862920392\n",
      "Interj Accuracy: 1.0\n",
      "Num Accuracy: 0.997871434653\n",
      "Ques Accuracy: 1.0\n",
      "Zero Accuracy: 1.0\n",
      "Dup Accuracy: 1.0\n",
      "Verb Accuracy: 0.994465730098\n",
      "Adj Accuracy: 0.970200085143\n",
      "Adv Accuracy: 0.984248616433\n",
      "Punc Accuracy: 1.0\n",
      "Conj Accuracy: 0.997871434653\n",
      "Det Accuracy: 0.990208599404\n",
      "Pron Accuracy: 0.990634312473\n",
      "Postp Accuracy: 0.99276287782\n",
      "Confusion matrix:\n",
      "  Tags: ['Noun', 'Interj', 'Num', 'Ques', 'Zero', 'Dup', 'Verb', 'Adj', 'Adv', 'Punc', 'Conj', 'Det', 'Pron', 'Postp']\n",
      "[[678   0   0   0   0   0   2  19   1   0   2   0   2   3]\n",
      " [  0   2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  16   0   0   0   0   0   0   0   0   5   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0 152   6   0   0   0   0   0   0]\n",
      " [ 17   0   0   0   0   0   4 236   3   0   0   3   2   2]\n",
      " [  2   0   0   0   0   0   0   7 152   0   0   4   7   8]\n",
      " [  0   0   0   0   0   0   0   0   0 530   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 129   0   3   0]\n",
      " [  0   0   0   0   0   0   0   4   3   0   0 191   4   0]\n",
      " [  3   0   0   0   0   0   0   1   0   0   0   0  72   0]\n",
      " [  0   0   0   0   0   0   0   2   2   0   0   0   0  68]]\n"
     ]
    }
   ],
   "source": [
    "t.print_acc(1)\n",
    "t.print_conf(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy is much higher in the known words case, presumably due to the fact that we are using the non-lemmatized versions of the words, which means that most of the words have only one actual tag that they are used as. The agglutinative nature of Turkish means there are a lot of unique word forms, which we are using as our vocabulary. These unique word forms end up having different PoS tags according to their suffixes. Obviously, this brings up our accuracy to levels that would not have been possible have we been using English.\n",
    "\n",
    "#### 6.1.3 All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "Overall Accuracy: 0.848199445983\n",
      "Noun Accuracy: 0.867590027701\n",
      "Interj Accuracy: 1.0\n",
      "Num Accuracy: 0.995290858726\n",
      "Ques Accuracy: 1.0\n",
      "Zero Accuracy: 1.0\n",
      "Dup Accuracy: 1.0\n",
      "Verb Accuracy: 0.928531855956\n",
      "Adj Accuracy: 0.946814404432\n",
      "Adv Accuracy: 0.97756232687\n",
      "Punc Accuracy: 1.0\n",
      "Conj Accuracy: 0.998614958449\n",
      "Det Accuracy: 0.993628808864\n",
      "Pron Accuracy: 0.993351800554\n",
      "Postp Accuracy: 0.995013850416\n",
      "Confusion matrix:\n",
      "  Tags: ['Noun', 'Interj', 'Num', 'Ques', 'Zero', 'Dup', 'Verb', 'Adj', 'Adv', 'Punc', 'Conj', 'Det', 'Pron', 'Postp']\n",
      "[[1513    0    0    0    0    0    2   19    1    0    2    0    2    3]\n",
      " [   0    2    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  12    0   16    0    0    0    0    0    0    0    0    5    0    0]\n",
      " [   0    0    0    1    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 246    0    0    0    0    0  152    6    0    0    0    0    0    0]\n",
      " [ 139    0    0    0    0    0    4  236    3    0    0    3    2    2]\n",
      " [  46    0    0    0    0    0    0    7  152    0    0    4    7    8]\n",
      " [   0    0    0    0    0    0    0    0    0  530    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0  129    0    3    0]\n",
      " [   0    0    0    0    0    0    0    4    3    0    0  191    4    0]\n",
      " [   5    0    0    0    0    0    0    1    0    0    0    0   72    0]\n",
      " [   1    0    0    0    0    0    0    2    2    0    0    0    0   68]]\n"
     ]
    }
   ],
   "source": [
    "t.print_acc(2)\n",
    "t.print_conf(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the effect of assigning noun to all unknown words in the confusion matrix. There is a trade-off between using word forms as our vocabulary and reaping the benefits of having one PoS tag each word, and having an abundance of unknown words, or using the word lemmas as our vocabulary and having more trouble tagging each word, while knowning more of the words. That is presumably why our training data contains different entries for word forms and word lemmas from time to time, to get the best of both words, but that also creates a difficulty in separating the word forms from their lemmas.\n",
    "\n",
    "### 6.2 POSTAG Results\n",
    "#### 6.2.1 Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "Overall Accuracy: 0.506740681998\n",
      "Card Accuracy: 0.991276764473\n",
      "Verb Accuracy: 0.840602696273\n",
      "Conj Accuracy: 1.0\n",
      "NPastPart Accuracy: 0.984139571768\n",
      "Ord Accuracy: 0.999206978588\n",
      "PersP Accuracy: 1.0\n",
      "DemonsP Accuracy: 0.999206978588\n",
      "ReflexP Accuracy: 0.999206978588\n",
      "Noun Accuracy: 0.506740681998\n",
      "Interj Accuracy: 1.0\n",
      "APastPart Accuracy: 0.990483743061\n",
      "Dup Accuracy: 1.0\n",
      "Adj Accuracy: 0.942902458366\n",
      "NInf Accuracy: 0.912767644726\n",
      "Punc Accuracy: 1.0\n",
      "Range Accuracy: 1.0\n",
      "NFutPart Accuracy: 0.992069785884\n",
      "Ques Accuracy: 1.0\n",
      "Adv Accuracy: 0.965107057891\n",
      "Prop Accuracy: 0.966693100714\n",
      "AFutPart Accuracy: 0.995241871531\n",
      "Postp Accuracy: 0.999206978588\n",
      "Distrib Accuracy: 1.0\n",
      "Real Accuracy: 1.0\n",
      "APresPart Accuracy: 0.980967486122\n",
      "Num Accuracy: 1.0\n",
      "Zero Accuracy: 0.947660586836\n",
      "QuesP Accuracy: 1.0\n",
      "Det Accuracy: 1.0\n",
      "Pron Accuracy: 1.0\n",
      "Confusion matrix:\n",
      "  Tags: ['Card', 'Verb', 'Conj', 'NPastPart', 'Ord', 'PersP', 'DemonsP', 'ReflexP', 'Noun', 'Interj', 'APastPart', 'Dup', 'Adj', 'NInf', 'Punc', 'Range', 'NFutPart', 'Ques', 'Adv', 'Prop', 'AFutPart', 'Postp', 'Distrib', 'Real', 'APresPart', 'Num', 'Zero', 'QuesP', 'Det', 'Pron']\n",
      "[[  0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 201   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 639   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  12   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  72   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 110   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  44   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  42   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   6   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  24   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  66   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "pos_tag_ind = cpar.tag_ind('postag')\n",
    "pos_hmm = hmm_train.HMM(cpar.tag_list(train_sentences, pos_tag_ind), pos_tag_ind)\n",
    "pos_hmm.train(train_sentences)\n",
    "ppt_sentences = hmm_tagger.pos_tag(hmm, test_sentences)\n",
    "\n",
    "pos_tester = Tester(pos_hmm.tags)\n",
    "pos_tester.build(test_sentences, ppt_sentences, pos_hmm.tag_ind, vocab = pos_hmm.vocab)\n",
    "\n",
    "pos_tester.print_acc(0)\n",
    "pos_tester.print_conf(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the overall accuracy has dropped when compared to the cpostag version, since the tags are more detailed and simply assigning one tag to all loses its power. It is an expected result of our approach.\n",
    "\n",
    "#### 6.2.2 Known Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "Overall Accuracy: 0.843763303533\n",
      "Card Accuracy: 0.991485738612\n",
      "Verb Accuracy: 0.988505747126\n",
      "Conj Accuracy: 0.997871434653\n",
      "NPastPart Accuracy: 0.996168582375\n",
      "Ord Accuracy: 0.999574286931\n",
      "PersP Accuracy: 0.984674329502\n",
      "DemonsP Accuracy: 0.993614303959\n",
      "ReflexP Accuracy: 0.997020008514\n",
      "Noun Accuracy: 0.945508727118\n",
      "Interj Accuracy: 1.0\n",
      "APastPart Accuracy: 0.994040017029\n",
      "Dup Accuracy: 1.0\n",
      "Adj Accuracy: 0.941677309493\n",
      "NInf Accuracy: 0.979991485739\n",
      "Punc Accuracy: 1.0\n",
      "Range Accuracy: 1.0\n",
      "NFutPart Accuracy: 0.998722860792\n",
      "Ques Accuracy: 1.0\n",
      "Adv Accuracy: 0.984248616433\n",
      "Prop Accuracy: 0.99276287782\n",
      "AFutPart Accuracy: 0.998297147722\n",
      "Postp Accuracy: 0.99276287782\n",
      "Distrib Accuracy: 1.0\n",
      "Real Accuracy: 1.0\n",
      "APresPart Accuracy: 0.97871434653\n",
      "Num Accuracy: 0.99318859089\n",
      "Zero Accuracy: 0.986802894849\n",
      "QuesP Accuracy: 0.997871434653\n",
      "Det Accuracy: 0.990208599404\n",
      "Pron Accuracy: 0.963814389102\n",
      "Confusion matrix:\n",
      "  Tags: ['Card', 'Verb', 'Conj', 'NPastPart', 'Ord', 'PersP', 'DemonsP', 'ReflexP', 'Noun', 'Interj', 'APastPart', 'Dup', 'Adj', 'NInf', 'Punc', 'Range', 'NFutPart', 'Ques', 'Adv', 'Prop', 'AFutPart', 'Postp', 'Distrib', 'Real', 'APresPart', 'Num', 'Zero', 'QuesP', 'Det', 'Pron']\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  15   0   0   5   0]\n",
      " [  0 138   0   0   0   0   0   0   1   0   0   0   6   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 129   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   7   0   0   0   2   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  36]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   7]\n",
      " [  0   1   2   0   0   0   0   0 596   0   0   0  16   0   0   0   0   0\n",
      "    0   0   0   3   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0  17   0   0   0 166   0   0   0   0   0\n",
      "    3   0   0   2   0   0   0   0   0   0   3   2]\n",
      " [  0   1   0   0   0   0   0   0  46   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 530   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   0   0   7   0   0   0   0   0\n",
      "  152   0   0   8   0   0   0   0   0   0   4   7]\n",
      " [  0   0   0   0   0   0   0   0  16   0   0   0   1   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    2   0   0  68   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  50   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  17   0   0   0   0   0   0  10   0   0   0   2   0   0   0   0   0\n",
      "    1   0   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   5]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0\n",
      "    3   0   0   0   0   0   0   0   0   0 191   4]\n",
      " [  0   0   0   0   0   0   0   0   3   0   0   0   1   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   9]]\n"
     ]
    }
   ],
   "source": [
    "pos_tester.print_acc(1)\n",
    "pos_tester.print_conf(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy is again down from the cpostag case. This again comes from the inherent difficulty of having more tags to choose from, plus the added fact that we have less data per tag. An accuracy of 84% is still high, though not up to the modern solutions.\n",
    "\n",
    "#### 6.2.3 All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "Overall Accuracy: 0.726038781163\n",
      "Card Accuracy: 0.991412742382\n",
      "Verb Accuracy: 0.936842105263\n",
      "Conj Accuracy: 0.998614958449\n",
      "NPastPart Accuracy: 0.991966759003\n",
      "Ord Accuracy: 0.99944598338\n",
      "PersP Accuracy: 0.990027700831\n",
      "DemonsP Accuracy: 0.995567867036\n",
      "ReflexP Accuracy: 0.997783933518\n",
      "Noun Accuracy: 0.792243767313\n",
      "Interj Accuracy: 1.0\n",
      "APastPart Accuracy: 0.992797783934\n",
      "Dup Accuracy: 1.0\n",
      "Adj Accuracy: 0.942105263158\n",
      "NInf Accuracy: 0.956509695291\n",
      "Punc Accuracy: 1.0\n",
      "Range Accuracy: 1.0\n",
      "NFutPart Accuracy: 0.996398891967\n",
      "Ques Accuracy: 1.0\n",
      "Adv Accuracy: 0.97756232687\n",
      "Prop Accuracy: 0.983656509695\n",
      "AFutPart Accuracy: 0.997229916898\n",
      "Postp Accuracy: 0.995013850416\n",
      "Distrib Accuracy: 1.0\n",
      "Real Accuracy: 1.0\n",
      "APresPart Accuracy: 0.979501385042\n",
      "Num Accuracy: 0.995567867036\n",
      "Zero Accuracy: 0.973130193906\n",
      "QuesP Accuracy: 0.998614958449\n",
      "Det Accuracy: 0.993628808864\n",
      "Pron Accuracy: 0.976454293629\n",
      "Confusion matrix:\n",
      "  Tags: ['Card', 'Verb', 'Conj', 'NPastPart', 'Ord', 'PersP', 'DemonsP', 'ReflexP', 'Noun', 'Interj', 'APastPart', 'Dup', 'Adj', 'NInf', 'Punc', 'Range', 'NFutPart', 'Ques', 'Adv', 'Prop', 'AFutPart', 'Postp', 'Distrib', 'Real', 'APresPart', 'Num', 'Zero', 'QuesP', 'Det', 'Pron']\n",
      "[[   0    0    0    0    0    0    0    0   11    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0   15    0    0\n",
      "     5    0]\n",
      " [   0  138    0    0    0    0    0    0  202    0    0    0    6    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0  129    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    3]\n",
      " [   0    0    0    0    0    0    0    0   27    0    0    0    2    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    1    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   36]\n",
      " [   0    0    0    0    0    0    0    0    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   15]\n",
      " [   0    0    0    0    0    0    0    0    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    7]\n",
      " [   0    1    2    0    0    0    0    0 1235    0    0    0   16    0\n",
      "     0    0    0    0    0    0    0    3    0    0    0    0    0    0\n",
      "     0    1]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0   12    0    0    0   14    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    1    0    0    0    0    0    0   89    0    0    0  166    0\n",
      "     0    0    0    0    3    0    0    2    0    0    0    0    0    0\n",
      "     3    2]\n",
      " [   0    1    0    0    0    0    0    0  156    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   530    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0   13    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0   46    0    0    0    7    0\n",
      "     0    0    0    0  152    0    0    8    0    0    0    0    0    0\n",
      "     4    7]\n",
      " [   0    0    0    0    0    0    0    0   58    0    0    0    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    6    0    0    0    4    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    1    0    0    0    2    0\n",
      "     0    0    0    0    2    0    0   68    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0   24    0    0    0   50    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0   17    0    0    0    0    0    0   76    0    0    0    2    0\n",
      "     0    0    0    0    1    0    0    0    0    0    0    0    0    0\n",
      "     0    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    5]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    4    0\n",
      "     0    0    0    0    3    0    0    0    0    0    0    0    0    0\n",
      "   191    4]\n",
      " [   0    0    0    0    0    0    0    0    3    0    0    0    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    9]]\n"
     ]
    }
   ],
   "source": [
    "pos_tester.print_acc(2)\n",
    "pos_tester.print_conf(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the combined accuracy is down due to the reasons explained above.\n",
    "\n",
    "## 7. Conclusion\n",
    "The 85% CPOSTAG accuracy over all words is definitely a good start for this very basic PoS Tagger. It shows how the HMM structure fits well to the problem. As stated above, there definitely is a trade-off between using word forms as our vocabulary and using the word lemmas as our vocabulary, due to the expected and empirical loss of accuracy when faced with unknown words. Also, our way of dealing with unknown words should definitely be replaced with another approach, since that is the primary thing that is keeping our accuracies down. If need be, we have a clear avenue to work on to increase our accuracy, and therefore, we can say that we are happy with the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
